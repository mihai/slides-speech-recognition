<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <title>Empowering Mobile Applications with Speech Recognition - Mihai Cîrlănaru</title>

    <link rel="stylesheet" href="css/reveal.css">
    <link rel="stylesheet" href="css/theme/night.css">
    <!-- Theme used for syntax highlighting of code -->
    <link rel="stylesheet" href="lib/css/atom-one-dark.css">
    <!-- Custom styles -->
    <link rel="stylesheet" href="css/styles.css">

    <!-- Printing and PDF exports -->
    <script>
      var link = document.createElement( 'link' );
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
      document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>
  </head>
  <body>
    <div class="reveal">
      <div class="slides">

<!-- SLIDE 1 -->
<section data-background-image="img/full-ink.jpg" data-transition="slide" data-state="intro">
  <h2>Empowering Mobile Applications with</h2>
  <h3 class="title-main">
    <span class="voice-visualizer">
      <svg preserveAspectRatio="none" id="visualizer" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
        <defs>
          <mask id="mask">
            <g id="maskGroup"></g>
          </mask>
          <linearGradient id="gradient" x1="0%" y1="0%" x2="0%" y2="100%">
            <stop offset="0%" style="stop-color:rgba(250, 250, 250, 1.0);stop-opacity:1" />
            <stop offset="20%" style="stop-color:rgba(215, 217, 221, 1.0);stop-opacity:1" />
            <stop offset="90%" style="stop-color:rgba(100, 101, 102, 1.0);stop-opacity:1" />
            <stop offset="100%" style="stop-color:rgba(56, 67, 79, 1.0);stop-opacity:1" />
          </linearGradient>
        </defs>
        <rect x="0" y="0" width="100%" height="100%" fill="url(#gradient)" mask="url(#mask)"></rect>
      </svg>
    </span>
    <span class="emphasize title-main__em">Speech Recognition</span>
  </h3>
  <div class="name">
    <div class="name-text">Mihai Cîrlănaru</div>
    <div class="name-sub">@MCirlanaru</div>
  </div>
  <div class="main">
    <div class="main-container">
      <h1 class="conf-logo">CodeMobile</h1>
    </div>
  </div>

  <aside class="notes">
    Thanks for having me here, I'm very excited to be in front of you today.
    While I am not a mobile developer by trade, I did have a few stints on Android development.
    <br><br>
    For the next 40min or so I will be talking to you about a topic I am quite passionate about, Speech Recognition.
    <b>WHY</b> it's a such a big deal these days, <b>HOW</b> it works, and <b>HOW</b> to use it on mobile (Android and iOS)<br><br>
    how many devs of each platform?
    assume the rest are web developers
    iPhone X is doing well as all iOS devs bought it to test/develop on the new form factor/layout.
  </aside>
</section>

<!-- SLIDE 2 -->
<section data-background-image="img/hudl-bg.jpg" data-transition="zoom-in fade-out">
  <div class="name-text">Mihai Cîrlănaru</div>
  <div class="name-sub">Senior Engineering Team Lead</div>
  <i class="hudl-logo"></i>
  <aside class="notes">
    First off, a bit about myself, my name is Mihai, almost did a PhD in NLP a few years back, but went to the
    dark side (joined a startup) and now I'm a team lead based in Germany, working on full-stack web for Hudl,
    a sports technology company that builds performance analysis tools for coaches and athletes.
  </aside>
</section>

<!-- <section data-background-image="img/hudl-video.mp4" data-background-video-loop=true>
  <aside class="notes">
    More than 143,000 total active teams across all sports and all levels
    Totals 5 million active users - coaches, athletes, trainers and analysts
    99% of American Football teams use Hudl, all 20 premier league teams use our software and 29/30 NBA teams...
  </aside>
</section> -->

 <!-- SLIDE 3 Intro -->
 <section data-background-image="img/title-slide-1.png" data-transition="slide">
   <p>&nbsp;</p>
   <aside class="notes">
     Speech Recognigion has been around for decades (early 1950s) but because it tackles such a HARD problem (just think of
     all the factors that play into it: accent, pronunciation, pitch, volume, and speed) it was mainly targetting
     highly specialized domains:<br>
     &nbsp;&nbsp;- speaker dependent: involves a training step where a user is asked to read some sentences -> fine tune<br>
     &nbsp;&nbsp;- command based/limited vocab: e.g. call routing, digit recognition<br>
     But then, technology caught up and starting with Apple's Siri in 2011 a wave a voice-enabled devices/personal
     assistants started to pick up.
   </aside>
 </section>

<!-- SLIDE 3 -->
<section data-background-image="img/siri.jpg" data-transition="fade-out">
  <p>&nbsp;</p>
  <aside class="notes">
    <i>But then, technology caught up and starting with Apple's Siri in 2011 a wave a voice-enabled devices/personal
    assistants started to pick up.</i><br>
    While not perfect back then, Siri opened the door to a wide range of voice-enabled interactions, making
    technology more intuitive and even fun.<br><br>
    How many of you used Siri so far?
  </aside>
</section>

<!-- SLIDE 4 -->
<section data-background-image="img/alexa-2.jpg" data-transition="fade-out">
  <p>&nbsp;</p>
  <aside class="notes">
    A few years later (2014), Amazon took the voice-enabled assistant to a whole new level, these black cylindrical
    speakers with the glowing blue LED ring quickly became the staple of home automation and pretty much
    kicked off a whole new market of IoT devices that can be controlled with one's voice.<br><br>
    How many of you used Alexa so far?
  </aside>
</section>

<!-- SLIDE 5 -->
<section data-background-image="img/google-home.webp" data-transition="fade-out">
  <p>&nbsp;</p>
  <aside class="notes">
    While Google did have voice input for search, it was only in 2016 that they created their own clever home decoration
    appliance in the shape of a flower-vase.<br><br>
    How many of you used Google voice input?<br><br>
    And as it turns out, more and more of these voice-enabled devices are coming up soon.
  </aside>
</section>

<!-- SLIDE 6 -->
<section data-background-image="img/voice-speakers.png" data-transition="fade-out">
  <p>&nbsp;</p>
  <aside class="notes">
    <i>And as it turns out, more and more of these voice-enabled devices are coming up soon</i><br>
    Drawing a parallel to the smartphone revolution, this might just be the Voice Speaker Revolution.<br><br>
    And as we saw here on a smaller scale, there are actually a lot of people using these -- in the US alone
  </aside>
</section>

<!-- SLIDE 7 -->
<section data-background-image="img/voice-users.png" data-transition="fade-out">
  <p>&nbsp;</p>
  <aside class="notes">
    <i>In the US alone</i> approx 55 million people use a voice enabled digital assistant,
    with the trend going upward.<br><br>
    But how come this exploded now?
  </aside>
</section>

<!-- SLIDE 8 -->
<section data-background-image="img/voice-performance.png" data-transition="fade-out">
  <p>&nbsp;</p>
  <aside class="notes">
    Well, speech recognition got very GOOD these days -- in terms of numbers (and I'll explain these in a bit)
    Google managed to improve their own speech recognizer's error rate from 23% to 4.9% in a handful of years and
    Baidu has currently knocked that out of the park with a 3.7% error rate achieved by its Deep Speech 2 system.
  </aside>
</section>

<!-- SLIDE 9 -->
<section data-background-image="img/performance.png" data-transition="fade-out">
  <p>&nbsp;</p>
  <aside class="notes">
    If you are wondering what this error rate actually means: it is the sum of all errors the system produces over
    the size of the reference speech. These errors can be Insertions, Substitutions and Deletions.
    <br><br>
    Then sure, you might say nice numbers and math, but what does that mean?
    It turns out speech recognition systems are so good that they can even beat humans to it.
    And there's research to prove it -- a group from Stanford…
  </aside>
</section>

<!-- SLIDE 10 -->
<section data-background-image="img/voice-research.png" data-transition="fade-out">
  <p>&nbsp;</p>
  <aside class="notes">
    A group from Stanford used the speech rec tech from Baidu (the 3.7% one from earlier) and tested it
    vs people at transcribing speech -- and guess what? it is better, 3x faster -- and if you check out the
    research at that link you will also learn that it performs better in terms of correctness for mandarin.<br><br>
    And if you used one of these devices you will not be surprised at all by these results --
    I personally have no idea how the app to set reminders or timers looks like, I just use Siri or Alexa.
    <br><br>
    Now that we've seen how big of deal it is let's see how it works
  </aside>
</section>

<!-- SLIDE 11 Intro -->
<section data-background-image="img/title-slide-2.png" data-transition="slide">
  <p>&nbsp;</p>
  <aside class="notes">
    Now that we've seen how big of deal it is let's see how it works
  </aside>
</section>

<!-- SLIDE 11 -->
<section data-background-image="img/speechwave-1.png" data-transition="fade">
  <p>&nbsp;</p>
  <aside class="notes">
    What our voice produces is a continuous sound wave whose amplitude varies based on the sounds involved.<br><br>
    The recognition process works as follows
  </aside>
</section>

<!-- SLIDE 12 -->
<section data-background-image="img/speechwave-2.png" data-transition="fade">
  <p>&nbsp;</p>
  <aside class="notes">
    Split the waveform in utterances (words or other non-linguistic sounds) by silences
  </aside>
</section>

<!-- SLIDE 13 -->
<section data-background-image="img/speechwave-3.png" data-transition="fade">
  <p>&nbsp;</p>
  <aside class="notes">
    Each utterance is split in frames and for each frame a feature vector is computed -- a set of numbers representing
    distinguishable properties of a sound
  </aside>
</section>

<!-- SLIDE 14 -->
<section data-background-image="img/speechwave-4.png" data-transition="fade">
  <p>&nbsp;</p>
  <aside class="notes">
    Using a series of models, the search space for a match is significantly reduced and thus recognition is made
    possible in a finite amount of time:<br>
    - <b>acoustic model</b> contains acoustic properties for each distinct unit of sound (senone)<br>
    - <b>phonetic dictionary</b> provides information on how to pronounce each word<br>
    - <b>language model</b> is used to restrict word search
  </aside>
</section>

<!-- SLIDE 14' -->
<section data-background-image="img/speechwave-5.png">
  <p>&nbsp;</p>
  <aside class="notes">
    And for the speech represented here the decoding would be "Hello World" -- just in case you wondered
    how speech recognizers are assessed<br><br>
    Now that we got this out of the way, let's get to the fun part
  </aside>
</section>

<!-- SLIDE 15 Intro -->
<section data-background-image="img/title-slide-3.png" data-transition="slide">
  <p>&nbsp;</p>
  <aside class="notes">
    <i>Now that we got this out of the way, let's get to the fun part.</i>
    And as every good talk should have a good story I'm going to tell you the one about how I got involved with
    Speech Recognition.<br><br>
    I started playing around with speech recognition a few years back when I was studying about it for
    my Masters degree and got this great opportunity…
  </aside>
</section>

<!-- SLIDE 15 -->
<section data-background-image="img/longtimeago.png" data-transition="fade">
  <p>&nbsp;</p>
  <aside class="notes">
    <i>I started playing around with speech recognition a few years back (5yr) when I was studying about it for
    my Masters degree and got this great opportunity</i> to put in practice what I learned about speech recognition
    within the Research Team at this small little tech company in the Valley
  </aside>
</section>


<!-- SLIDE 16 -->
<section data-background-image="img/mozilla.jpg" data-transition="fade">
  <i class="mozilla-logo"></i>
  <aside class="notes">
    <i>Within the Research Team at Mozilla,</i> to build a speech recognition engine for the now defunct Firefox OS
  </aside>
</section>

<!-- SLIDE 16' -->
<section data-background-image="img/firefox-os3.jpg" data-transition="fade">
  <aside class="notes">
    Mozilla’s mobile phone operating system built entirely using web technologies. RIP it was a great<br><br>
    Being a research project, the focus was more on exploring what is even possible in terms of speech recognition
    on mobile (using web tech), rather than have something production ready.
  </aside>
</section>

<!-- SLIDE 17 -->
<section data-background-image="img/full-ink.jpg">
  <div class="name-text">DEMO</div>
  <div class="name-sub emphasize">github.com/mihai/talk.js</div>
  <i class="github-repo hero-pic">&nbsp;</i>

  <aside class="notes">
    During my time there, I have built a pure JavaScript speech recognizer that performs the decoding process
    on the client, without the need of any cloud-based backend to send the speech to for decoding.<br>
    And I'm going to show it to you -- to make it even more exciting I'm going to kill the network on this --
    don't try this in live demos
    <br><br>
    If you are interested more on how this was built, and how it works behind the scenes,
    feel free to grab me after the talk.
  </aside>
</section>

<!-- SLIDE 18 -->
<section data-background-image="img/mobile-split-front.png" data-transition="fade">
  <aside class="notes">
    And while I was focusing on the wrong mobile platform, the big players were already playing around with Speech
    Recognition to enhance the user experience of writing messages:<br><br>
    Android - June 2010 (API level 8)<br>
    iOS - 2011 keyboard dictation (iOS 5), API with iOS 10 (Sept 2016)<br><br>
    To please everyone I went with this split screen approach, that allows you to see how both platforms do it.
  </aside>
</section>

<section data-background-image="img/mobile-split-content-small.png" data-transition="fade">
  <h2 class="split-slide-title">Speech Recognition API</h2>
  <div class="inner-container">
  <div class="pane">
    <div class="name-sub emphasize">bit.ly/AndroidSpeechAPI</div>
<pre>
<code data-trim data-noescape class="java">
// Speech Recognition package
// on Android (API level 8+)
import android.speech.*

// Relevant class
class SpeechRecognizer {}

// Getting a recognizer
myRecognizer = SpeechRecognizer
  .createSpeechRecognizer(context)
</code>
</pre>
  </div>
  <div class="pane">
    <div class="name-sub emphasize">bit.ly/iOSSpeechAPI</div>
<pre>
<code data-trim data-noescape class="swift">
// Speech Recognition framework
// on iOS (10.0+)
import Speech

// Relevant class
class SFSpeechRecognizer {}

// Getting a recognizer
myRecognizer = SFSpeechRecognizer()
</code>
</pre>
</div></div>
  <aside class="notes">
    The relevant packages, class and how to instantiate one. Default uses the devices current locale.
  </aside>
</section>

<section data-background-image="img/mobile-split-content-small.png" data-transition="fade">
  <h2 class="split-slide-title">Permissions</h2>
  <div class="inner-container">
  <div class="pane">
    <div class="name-sub emphasize">bit.ly/AndroidSpeechAPI</div>
<pre>
<code data-trim data-noescape class="java">
android.permission.RECORD_AUDIO

// Required if performing recognition with
// the cloud based service
android.permission.INTERNET
</code>
</pre>
  </div>
  <div class="pane">
    <div class="name-sub emphasize">bit.ly/iOSSpeechAPI</div>
<pre>
<code data-trim data-noescape class="swift">
// Relevent authStatus:
//    .authorized
//    .denied
//    .restricted
SFSpeechRecognizer.requestAuthorization() {
  (authStatus) in …
}
</code>
</pre>
</div></div>
  <aside class="notes">
    Before starting any recognition you must ensure that the user has authorized it.
  </aside>
</section>

<section data-background-image="img/mobile-split-content-small.png" data-transition="fade">
  <h2 class="split-slide-title">Start Recognition</h2>
  <div class="inner-container">
  <div class="pane">
    <div class="name-sub emphasize">bit.ly/AndroidSpeechAPI</div>
<pre>
<code data-trim data-noescape class="java">
// Requires a RecognizerIntent that
// contains recognition parameters:
// - ACTION_RECOGNIZE_SPEECH
//   sets its action
// - EXTRA_LANGUAGE_PREFERENCE
//   language to recognize
// …
myRecognizer.startListening(intent)
</code>
</pre>
  </div>
  <div class="pane">
    <div class="name-sub emphasize">bit.ly/iOSSpeechAPI</div>
<pre>
<code data-trim data-noescape class="swift">
// Needs a Speech Recognition request, can be
// SFSpeechAudioBufferRecognitionRequest
// for live audio (with AVAudioEngine) or
// SFSpeechURLRecognitionRequest
// for recognizing from file
myRecognizer
  .recognitionTask(with: request) {
    (result, error) in …
  }
</code>
</pre>
</div></div>
  <aside class="notes">
    Android - The intent allows you to define the locale and other configuration settings (i.e. how many results to return).<br>
    iOS - The completion handler can be called more than once as speech is recognized incrementally -- you can check with isFinal
  </aside>
</section>

<section data-background-image="img/mobile-split-content-small.png" data-transition="fade">
  <h2 class="split-slide-title">Get Recognition Results</h2>
  <div class="inner-container">
  <div class="pane">
    <div class="name-sub emphasize">bit.ly/AndroidSpeechAPI</div>
<pre>
<code data-trim data-noescape class="java">
// Catch results with onActivityResult
@Override
protected void onActivityResult(
  int requestCode, int resultCode,
  Intent data
) {
  //…
  switch (requestCode) {
  case REQ_CODE_SPEECH_INPUT: {
    if (resultCode == RESULT_OK && null != data) {
      ArrayList<String> result = data
        .getStringArrayListExtra(
          RecognizerIntent.EXTRA_RESULTS
        );
      System.out.println(
        result.get(0)
      );
    }
//…
</code>
</pre>
  </div>
  <div class="pane">
    <div class="name-sub emphasize">bit.ly/iOSSpeechAPI</div>
<pre>
<code data-trim data-noescape class="swift">
// Within recognitionTask handler
guard let result = result else {
  // Recognition failed
  return
}
if result.isFinal {
  // Print the recognized speech
  print(result
    .bestTranscription
    .formattedString
  )
}
</code>
</pre>
</div></div>
  <aside class="notes">
    Android - catch the results on the activity, check the intent<br>
    iOS - within the completion handler
  </aside>
</section>

<section data-background-image="img/mobile-split-content-small.png" data-transition="fade">
  <h2 class="split-slide-title">Stop Recognition</h2>
  <div class="inner-container">
  <div class="pane">
    <div class="name-sub emphasize">bit.ly/AndroidSpeechAPI</div>
<pre>
<code data-trim data-noescape class="java">
myRecognizer.stopListening()
</code>
</pre>
  </div>
  <div class="pane">
    <div class="name-sub emphasize">bit.ly/iOSSpeechAPI</div>
<pre>
<code data-trim data-noescape class="swift">
// Stop the speech recognition request
// Also, don't forget about the AVAudioEngine
// instance (if live audio was used)
// …
request.stop()
</code>
</pre>
</div></div>
  <aside class="notes">
    Free up resources on close or cancel.<br>
    Proverbial black and white difference between the too mobile platforms, there is the middle ground -- the web platform,
    and there is an API for it.
  </aside>
</section>

<section>
<section data-background-image="img/mobile-split-html5.png" data-transition="fade">
  <aside class="notes">
    While the web is a bit slow to push standards around with so many browser vendors and so much fragmentation
    (Hello Android), it is slowly catching up.
  </aside>
</section>

<section data-background-image="img/full-ink.jpg">
  <div class="name-text">Web Speech API</div>
  <div class="name-sub emphasize">bit.ly/WebSpeechAPI</div>

<pre>
<code data-trim data-noescape>
const SpeechRecognition = SpeechRecognition || webkitSpeechRecognition;
let recognizer = null;

if (!SpeechRecognition) {
  console.error("Web Speech APIs not supported in your browser");
} else {
  recognizer = new SpeechRecognition();
}
</code>
</pre>
  <aside class="notes">
    You first need to instantiate a Speech Recognition object, and since the APIs are still experimental they
    do need the browser prefix for Chrome (not for Firefox though).
  </aside>
</section>

<section data-background-image="img/full-ink.jpg">
  <div class="name-text">Speech Recognition Attributes</div>
  <div class="name-sub emphasize">bit.ly/WebSpeechAPI</div>
<pre>
<code data-trim data-noescape>
// Sets the language to be recognized (32 languages
// supported, incl. Romanian)
recognizer.lang = 'en-US';

// Get recognition results as early as possible,
// even if they will change
recognizer.interimResults = true;

// Continuously listen to speech, regardless if
// the user takes pauses or not
recognizer.continuous = true;

// The number of alternative recognition
// matches to be returned
recognizer.maxAlternatives = 3;

…
</code>
</pre>
  <aside class="notes">
    Some of the most relevant configuration attributes of the speech recognition instance.
  </aside>
</section>

<section data-background-image="img/full-ink.jpg">
  <div class="name-text">Speech Recognition Event Handlers</div>
  <div class="name-sub emphasize">bit.ly/WebSpeechAPI</div>
<pre>
<code data-trim data-noescape>
recognizer.onresult // Whenever a speech recognition match is found
recognizer.onnomatch // When no match was found for the current speech
recognizer.onerror // When an error occurred
…
</code>
</pre>
  <aside class="notes">
    There are a few event handlers provided for the Speech Recognition interface, and I would recommend you check
    the full API at http://bit.ly/WebSpeechAPI, however the minimum needed to get you started are:<br>
    onresult, onnomatch, onerror
  </aside>
</section>

<section data-background-image="img/full-ink.jpg">
  <div class="name-text">Speech Recognition Control</div>
  <div class="name-sub emphasize">bit.ly/WebSpeechAPI</div>
<pre>
<code data-trim data-noescape class="js">
recognizer.start();
recognizer.abort();
recognizer.stop();
</code>
</pre>
  <aside class="notes">
    In terms of controlling the recognizer we have a pretty self-explanatory API: start and stop.<br>
    And one last piece to the puzzle, this is how a Speech Recognition onResult event format looks like.
  </aside>
</section>

<!-- SLIDE 23 -->
<section data-background-image="img/full-ink.jpg">
  <div class="name-text">Speech Recognition Result format</div>
  <div class="name-sub emphasize">bit.ly/WebSpeechAPI</div>
<pre>
<code data-trim data-noescape class="js">
// results[i][0].transcript -- top confidence transcription for result i
{
  results: [ // SpeechRecognitionResultList
    [ // SpeechRecognitionResult
      { // SpeechRecognitionAlternative
        transcript: "hello",
        confidence: 0.8999,
        isFinal: false
      },
      { // SpeechRecognitionAlternative
        transcript: "world",
        confidence: 0.4792,
        isFinal: false
      }
    ],
  ]
  …
}
</code>
</pre>
  <aside class="notes">
    SpeechRecognitionResultList -- each result has multiple alternatives results[0][0].transcript to get the
    first result with highest probability<br>
    isFinal -- this result is not an interim result and thus it will not change
  </aside>
</section>

<section data-background-image="img/full-ink.jpg">
  <div class="name-text">Privacy</div>
  <div class="name-sub emphasize">bit.ly/WebSpeechAPI</div>
  <i class="privacy hero-pic">&nbsp;</i>
  <aside class="notes">
    In terms of privacy, Chrome needs to ask the user for permission to use the microphone, in which case onstart only
    fires when and if the user allows permission.<br>
    Pages hosted on HTTPS do not need to ask repeatedly for permission, whereas HTTP hosted pages do.<br>
    And this brings us to the last point about the WebSpeech API
  </aside>
</section>

</section>

<section data-background-image="img/full-ink.jpg" data-transition="slide-in fade-out">
  <div class="name-text">Browser Support</div>
  <div class="name-sub emphasize">caniuse.com/#feat=speech-recognition</div>
  <i class="browser-support hero-pic">&nbsp;</i>
  <aside class="notes">
    <i>While the web is a bit slow to push standards around with so many browser vendors and so much fragmentation
    (Hello Android), it is slowly catching up.</i><br>
    As you can see support is not that great, it’s only Chrome and Opera that support it out of the box,
    whereas Firefox needs the webspeech recognition flag set. Microsoft is currently working on it for Edge.
    <br><br>
    And before you all fall asleep, I'm going to highlight some best practices around using Speech Recognition
    on any platform.
  </aside>
</section>

<section data-background-image="img/mobile-ux.png" data-transition="fade">
  <div class="inner-container">
    <div class="pane"></div>
    <div class="pane">
      <h2>Best practices</h2>
      <ul class="list">
        <li class="fragment fade-right">Handle <span class="emphasize">failures</span></li>
        <li class="fragment fade-right">Plan for short recordings (<span class="emphasize">1 min limit</span>)</li>
        <li class="fragment fade-right"><span class="emphasize">Remind the user</span> your app is recording</li>
        <li class="fragment fade-up">Do <em>not</em> perform speech recognition on <span class="emphasize">private or sensitive information</span></li>
      </div>
    </ul>
  </div>
  <aside class="notes">
    Be prepared to <b>handle the failures</b> that can be caused by reaching speech recognition limits. Because speech recognition is a network-based service, limits are enforced so that the service can remain freely available to all apps.
    <br><br>
    Plan for a <b>one-minute limit</b> on audio duration. Speech recognition can place a relatively high burden on battery life and network usage. In iOS 10, utterance audio duration is limited to about one minute, which is similar to the limit for keyboard-related dictation.
    <br><br>
    <b>Remind the user</b> when your app is recording. For example, you can play "now recording" sounds and display a visual indicator that helps users understand that they're being actively recorded. You can also display speech as it is being recognized so that users understand what your app is doing and when recognition errors occur.
    <br><br>
    Do not perform speech recognition on <b>private or sensitive information</b>. Some speech is simply not appropriate for recognition. Avoid sending passwords, health or financial data, and other sensitive speech for recognition.
    <br><br>
    And on that note…
  </aside>
</section>

<!-- SLIDE 26 -->
<section data-background-image="img/hudl-layout-main.png" data-transition="fade" data-state="voice-visualizer speech-start">
  <h3><span class="voice-visualizer">
    <svg preserveAspectRatio="none" id="visualizer-end" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
      <defs>
        <mask id="mask-end">
          <g id="maskGroup"></g>
        </mask>
        <linearGradient id="gradient-end" x1="0%" y1="0%" x2="0%" y2="100%">
          <stop offset="0%" style="stop-color:rgba(250, 250, 250, 1.0);stop-opacity:1" />
          <stop offset="20%" style="stop-color:rgba(215, 217, 221, 1.0);stop-opacity:1" />
          <stop offset="90%" style="stop-color:rgba(255, 140, 0, 1.0);stop-opacity:1" />
          <stop offset="100%" style="stop-color:rgba(255, 99, 0, 1.0);stop-opacity:1" />
        </linearGradient>
      </defs>
      <rect x="0" y="0" width="100%" height="100%" fill="url(#gradient-end)" mask="url(#mask-end)"></rect>
    </svg>
  </span></h3>
  <div id="results">
    <span id="final_span" class="final"></span>
    <span id="interim_span" class="interim"></span>
  </div>
  <div class="name">
    <div class="name-text">Mihai Cîrlănaru</div>
    <div class="name-sub">@MCirlanaru</div>
  </div>
  <div class="main">
    <div class="main-container">
      <h1 class="conf-logo-secondary">CodeMobile</h1>
    </div>
  </div>
  <aside class="notes">
    I hope you are as excited as I am about using Voice in your applications, chances are high we are experiencing
    the next big user interaction paradigm - Thank you
  </aside>
</section>

      </div>
    </div>

<script src="lib/js/head.min.js"></script>
<script src="js/reveal.js"></script>

<script>

var paths = document.getElementsByTagName('path');
var visualizer = document.getElementById('visualizer');
var visualizerEnd = document.getElementById('visualizer-end');
var mask1 = visualizer.getElementById('mask');
var mask2 = visualizerEnd.getElementById('mask-end');
var path;
var NR_LINES = 40;

var soundAllowedCallback = function (stream) {
    //Audio stops listening in FF without // window.persistAudioStream = stream;
    //https://bugzilla.mozilla.org/show_bug.cgi?id=965483
    //https://support.mozilla.org/en-US/questions/984179
    window.persistAudioStream = stream;
    var audioContent = new AudioContext();
    var audioStream = audioContent.createMediaStreamSource( stream );
    var analyser = audioContent.createAnalyser();
    audioStream.connect(analyser);
    analyser.fftSize = 1024;

    var frequencyArray = new Uint8Array(analyser.frequencyBinCount);
    visualizer.setAttribute('viewBox', `0 0 ${NR_LINES} 255`);
    visualizerEnd.setAttribute('viewBox', `0 0 ${NR_LINES} 255`);

    //Through the frequencyArray has a length longer than 255, there seems to be no
    //significant data after this point. Not worth visualizing.
    for (var i = 0 ; i < NR_LINES; i++) {
        path1 = document.createElementNS('http://www.w3.org/2000/svg', 'path');
        path2 = document.createElementNS('http://www.w3.org/2000/svg', 'path');
        mask1.appendChild(path1);
        mask2.appendChild(path2);
    }
    var doDraw = function () {
        requestAnimationFrame(doDraw);
        analyser.getByteFrequencyData(frequencyArray);
        var adjustedLength;
        for (var i = 0 ; i < NR_LINES; i++) {
            adjustedLength = Math.floor(frequencyArray[i]) - (Math.floor(frequencyArray[i]) % 5);
            paths[i].setAttribute('d', 'M '+ (i) +',255 l 0,-' + adjustedLength);
        }

    }
    doDraw();
}

navigator.getUserMedia({ audio: true }, soundAllowedCallback, function(){});

var two_line = /\n\n/g;
var one_line = /\n/g;
var thank_you = /\s+thank\s+you/gi;
function linebreak(s) {
  return s.replace(two_line, '<p></p>').replace(one_line, '<br>');
}
var first_char = /\S/;
function capitalize(s) {
  return s.replace(first_char, function(m) {
    return m.toUpperCase();
  }).replace(thank_you, '<p class="thanks">thank you</p>');
}

var final_transcript = '';
var final_span = document.getElementById('final_span');
var interim_span = document.getElementById('interim_span');
var listeningToSpeech = false;
var recognizerTimeout = null;
var recognizer = {};

var isSpeakerNotes = document.URL.indexOf('?receiver') !== -1;

function resetSpeech() {
  final_transcript = '';
  listeningToSpeech = false;
  final_span.innerHTML = '';
  interim_span.innerHTML = '';

  if (!isSpeakerNotes) {
    // Setup speech recognition
    if (recognizer.abort) recognizer.abort();

    recognizer = new webkitSpeechRecognition();
    recognizer.continuous = true;
    recognizer.lang = "en-US";
    recognizer.maxAlternatives = 3;
    recognizer.interimResults = true;
  }
}

// Initialize speech recognizer
resetSpeech();

// More info https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({
  history: true,
  controls: false,
  viewDistance: 5,
  keyboard: {
    // ENTER KEY
    13: function() {
      if (!listeningToSpeech) {
        console.info('KEY SPEECH START');
        recognizer.start();
        listeningToSpeech = true;
      } else {
        console.info('KEY SPEECH END');
        recognizer.stop();
        listeningToSpeech = false;
      }
    },
    // SPACE KEY
    32: function() {
      recognizer.abort();
      final_transcript = '';
      listeningToSpeech = false;
      final_span.innerHTML = '';
      interim_span.innerHTML = '';

      console.warn('RESET SPEECH RECOGNIZER');

      recognizerTimeout = setTimeout(function(){
        console.info('SPEECH STARTING...');
        recognizer.start();
        listeningToSpeech = true;
      }, 500);
    },
    // UP KEY
    38: 'prev',
    40: 'next'
  },
  // width: 1920,
  // height: 1080,
  width: 1280,
  height: 720,

  // More info https://github.com/hakimel/reveal.js#dependencies
  dependencies: [
    { src: 'plugin/markdown/marked.js' },
    { src: 'plugin/markdown/markdown.js' },
    { src: 'plugin/notes/notes.js', async: true },
    { src: 'plugin/zoom-js/zoom.js', async: true },
    { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
  ]
});

if (!isSpeakerNotes) {
  recognizer.onend = function(event) {
    console.warn('SPEECH END');
    listeningToSpeech = false;
    final_transcript += ' ';
  }

  recognizer.onresult = function(event) {
    var interim_transcript = '';
    for (var i = event.resultIndex; i < event.results.length; ++i) {
      if (event.results[i].isFinal) {
        final_transcript += event.results[i][0].transcript;
      } else {
        interim_transcript += event.results[i][0].transcript;
      }
    }
    final_transcript = capitalize(final_transcript);
    final_span.innerHTML = linebreak(final_transcript);
    interim_span.innerHTML = linebreak(interim_transcript);

    console.group('SPEECH RESULT');
    console.info('INTERIM:', interim_transcript);
    console.info('FINAL:', final_transcript);
    console.groupEnd();
  };

  recognizer.onerror = function(event) {
    console.error('ERROR:', event.error);
    if (event.error === 'network' || event.error === 'aborted') {
      console.info('SPEECH RESTART');
      recognizer.stop();
      if (recognizerTimeout) {
        clearTimeout(recognizerTimeout);
        recognizerTimeout = null;
      }
      recognizerTimeout = setTimeout(function(){
        console.info('SPEECH STARTING...');
        recognizer.start();
        listeningToSpeech = true;
      }, 500);
    }
  };

  Reveal.addEventListener( 'slidechanged', function( event ) {
    // event.previousSlide, event.currentSlide, event.indexh, event.indexv
    console.info('SLIDE CHANGED', event.indexh);
  } );

  Reveal.addEventListener( 'speech-start', function(ev) {
    // Start recognition
    recognizer.start();
    listeningToSpeech = true;
    console.info('SPEECH START SLIDE')
  }, false );

  Reveal.addEventListener( 'intro', function(ev) {
    var visualizer = document.getElementById('visualizer');
    paths = document.getElementsByTagName('path');
  }, false );

  Reveal.addEventListener( 'voice-visualizer', function(ev) {
    var visualizer = document.getElementById('visualizer-end');
    paths = visualizer.getElementsByTagName('path');
  }, false );
}
</script>
  </body>
</html>

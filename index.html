<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <title>Speech Recognition on the Web - Mihai Cîrlănaru</title>

    <link rel="stylesheet" href="css/reveal.css">
    <link rel="stylesheet" href="css/theme/night.css">
    <!-- Theme used for syntax highlighting of code -->
    <link rel="stylesheet" href="lib/css/zenburn.css">
    <!-- Custom styles -->
    <link rel="stylesheet" href="css/styles.css">

    <!-- Printing and PDF exports -->
    <script>
      var link = document.createElement( 'link' );
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
      document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>
  </head>
  <body>
    <div class="reveal">
      <div class="slides">

<!-- SLIDE 1 -->
<section data-background-image="img/full-ink.jpg" data-transition="slide" data-state="intro">
  <h2>Speech Recognition</h2>
  <h3 class="title-main">
    <span class="voice-visualizer">
      <svg preserveAspectRatio="none" id="visualizer" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
        <defs>
          <mask id="mask">
            <g id="maskGroup"></g>
          </mask>
          <linearGradient id="gradient" x1="0%" y1="0%" x2="0%" y2="100%">
            <stop offset="0%" style="stop-color:rgba(250, 250, 250, 1.0);stop-opacity:1" />
            <stop offset="20%" style="stop-color:rgba(215, 217, 221, 1.0);stop-opacity:1" />
            <stop offset="90%" style="stop-color:rgba(100, 101, 102, 1.0);stop-opacity:1" />
            <stop offset="100%" style="stop-color:rgba(56, 67, 79, 1.0);stop-opacity:1" />
          </linearGradient>
        </defs>
        <rect x="0" y="0" width="100%" height="100%" fill="url(#gradient)" mask="url(#mask)"></rect>
      </svg>
    </span>
    on the <span class="emphasize title-main__em">Web</span>
  </h3>
  <div class="name">
    <div class="name-text">Mihai Cîrlănaru</div>
    <div class="name-sub">@MCirlanaru</div>
  </div>
  <div class="main">
    <div class="main-container">
      <i class="conf-logo"></i>
    </div>
  </div>

  <aside class="notes">
    Thanks for having me here, it’s amazing to be back home in front of such a big crowd of enthusiasts.

    For the next half an hour I will be talking to you about a topic I am very passionate about, Speech Recognition. And most excitingly of all, about how to empower your web application with voice-triggered interactions.
  </aside>
</section>

<!-- SLIDE 2 -->
<section data-background-image="img/hudl-bg.jpg" data-transition="zoom-in fade-out">
  <div class="name-text">Mihai Cîrlănaru</div>
  <div class="name-sub">Software Development Lead</div>
  <i class="hudl-logo"></i>
  <aside class="notes">
    First off, a bit about myself, my name is Mihai Cirlanaru, I am based in London, working on full-stack web for Hudl, a sports technology company that builds analysis tools for coaches and athletes at all levels to help them win.
  </aside>
</section>

<!-- <section data-background-image="img/hudl-video.mp4" data-background-video-loop=true>
  <aside class="notes">
    More than 143,000 total active teams across all sports and all levels
    Totals 5 million active users - coaches, athletes, trainers and analysts
    99% of American Football teams use Hudl, all 20 premier league teams use our software and 29/30 NBA teams...
  </aside>
</section> -->

<!-- SLIDE 3 -->
<section data-background-image="img/siri.jpg" data-transition="slide">
  <p>&nbsp;</p>
  <aside class="notes">
    In case you don't already use personal assistant apps like siri, Speech recognition:
    Speaker independent: regardless of the speaker
    Speaker dependent: involves a training step where a user is asked to read some sentences -> fine tune

    “Voice User Interface” -- command based/limited vocabulary
      Voice dialing
      Call routing
      Simple data entry
    Dictation/Notes taking -- free form speech
  </aside>
</section>

<!-- SLIDE 4 -->
<section data-background-image="img/alexa-2.jpg" data-transition="slide">
  <p>&nbsp;</p>
  <aside class="notes">
    In case you don't already use personal assistant apps like siri, Speech recognition:
    Speaker independent: regardless of the speaker
    Speaker dependent: involves a training step where a user is asked to read some sentences -> fine tune

    “Voice User Interface” -- command based/limited vocabulary
      Voice dialing
      Call routing
      Simple data entry
    Dictation/Notes taking -- free form speech
  </aside>
</section>

<!-- SLIDE 5 -->
<section data-background-image="img/google-home.webp" data-transition="slide">
  <p>&nbsp;</p>
  <aside class="notes">
    In case you don't already use personal assistant apps like siri, Speech recognition:
    Speaker independent: regardless of the speaker
    Speaker dependent: involves a training step where a user is asked to read some sentences -> fine tune

    “Voice User Interface” -- command based/limited vocabulary
      Voice dialing
      Call routing
      Simple data entry
    Dictation/Notes taking -- free form speech
  </aside>
</section>

<!-- SLIDE 6 -->
<section data-background-image="img/voice-speakers.png" data-transition="slide" data-background-transition="slide">
  <p>&nbsp;</p>
  <aside class="notes">
  </aside>
</section>

<!-- SLIDE 7 -->
<section data-background-image="img/voice-users.png" data-transition="slide" data-background-transition="slide">
  <p>&nbsp;</p>
  <aside class="notes">
  </aside>
</section>

<!-- SLIDE 8 -->
<section data-background-image="img/voice-performance.png" data-transition="slide" data-background-transition="slide">
  <p>&nbsp;</p>
  <aside class="notes">
  </aside>
</section>

<!-- SLIDE 9 -->
<section data-background-image="img/voice-research.png" data-transition="slide" data-background-transition="slide">
  <p>&nbsp;</p>
  <aside class="notes">
  </aside>
</section>

<!-- SLIDE 10 -->
<section data-background-image="img/performance.png" data-transition="slide" data-background-transition="slide">
  <p>&nbsp;</p>
  <aside class="notes">
    This is how you assess it - Performance measured in terms of:
    Accuracy: word error rate (WER) = (Substitutions + Deletions + Insertions)/Number words in reference speech
    Speed: real time factor = ratio between how long it took to decode the speech and how long it actually was
  </aside>
</section>

<!-- SLIDE 11 -->
<section data-background-image="img/speechwave-1.png" data-transition="fade">
  <p>&nbsp;</p>
  <aside class="notes">
    A continuous sound wave whose amplitude varies based on the sounds involved. The recognition process works as follows
  </aside>
</section>

<!-- SLIDE 12 -->
<section data-background-image="img/speechwave-2.png" data-transition="fade">
  <p>&nbsp;</p>
  <aside class="notes">
    Split the waveform in utterances (words or other non-linguistic sounds) by silences
  </aside>
</section>

<!-- SLIDE 13 -->
<section data-background-image="img/speechwave-3.png" data-transition="fade">
  <p>&nbsp;</p>
  <aside class="notes">
    Each utterance is split in frames
    For each frame a feature vector is computed -- a set of numbers representing distinguishable properties of a sound
  </aside>
</section>

<!-- SLIDE 14 -->
<section data-background-image="img/speechwave-4.png" data-transition="fade">
  <p>&nbsp;</p>
  <aside class="notes">
    Using a series of models, the search space for a match is significantly reduced and thus recognition is made possible in a finite amount of time:
    - acoustic model contains acoustic properties for each distinct unit of sound (senone)
    - phonetic dictionary contains a mapping from words to phones
    - language model is used to restrict word search
  </aside>
</section>

<!-- SLIDE 14' -->
<section data-background-image="img/speechwave-5.png">
  <p>&nbsp;</p>
  <aside class="notes">
    And for the speech represented here the decoding would be "Hello World" -- just in case you wondered how speech recognizers are assessed
  </aside>
</section>

<!-- SLIDE 15 -->
<section data-background-image="img/longtimeago.png" data-transition="fade">
  <p>&nbsp;</p>
  <aside class="notes">
    I started playing around with speech recognition about 4 years ago when I was studying about it for my Masters degree and was quite intrigued by how hard is the problem it solves -- all the various factors like accent, pronunciation, articulation, roughness, nasality, pitch, volume, and speed influence it.

    In practice, speech looks a lot like this:
  </aside>
</section>


<!-- SLIDE 16 -->
<section data-background-image="img/mozilla.jpg" data-transition="fade">
  <i class="mozilla-logo"></i>
  <aside class="notes">
    Towards the end of my studies, I had the great opportunity to put in practice what I learned about speech recognition within the Research Team at Mozilla, to build a speech recognition engine for Firefox OS
  </aside>
</section>

<!-- SLIDE 16' -->
<section data-background-image="img/firefox-os3.jpg" data-transition="fade">
  <aside class="notes">
    -- Mozilla’s mobile phone operating system built entirely using web technologies. Being a research project, the focus was more on exploring what is even possible in terms of speech recognition for the web, rather than have something production ready.
  </aside>
</section>

<!-- SLIDE 17 -->
<section data-background-image="img/full-ink.jpg">
  <div class="name-text">DEMO</div>
  <div class="name-sub emphasize">github.com/mihai/talk.js</div>
  <i class="github-repo hero-pic">&nbsp;</i>

  <aside class="notes">
    During my time there, I have built a pure JavaScript speech recognizer that performs the decoding process on the client, without the need of any cloud-based backend to send the speech to for decoding. And here’s a demo for it:

    If you are interested more on how this was built, and how it works behind the scenes, feel free to grab me after the talk.
  </aside>
</section>

<!-- SLIDE 18 -->
<section data-background-image="img/full-ink.jpg" data-state="speech-end">
  <div class="name-text">Web Speech API Spec</div>
  <div class="name-sub emphasize">bit.ly/WebSpeechSpec</div>
  <i class="spec hero-pic">&nbsp;</i>
  <aside class="notes">
    Little did I know when researching this at Mozilla, at around the same time, a team from Google were writing a spec for Web Speech APIs.

    defining JavaScript APIs that would allow developers to incorporate speech recognition into their web pages. About a year later, with Chrome v25, Web Speech APIs were out.

    Whereas Chrome had speech input capabilities with x-webkit-speech attribute (now deprecated) to input fields since v11.

    Ok, so let’s see how these APIs look like in practice
  </aside>
</section>

<!-- SLIDE 19 -->
<section data-background-image="img/full-ink.jpg" data-state="speech-start">
  <div class="name-text">Web Speech API</div>
  <div class="name-sub emphasize">bit.ly/WebSpeechAPI</div>

<pre>
<code data-trim data-noescape>
const SpeechRecognition = SpeechRecognition || webkitSpeechRecognition;
let recognizer = null;

if (!SpeechRecognition) {
  console.error("Web Speech APIs not supported in your browser");
} else {
  recognizer = new SpeechRecognition();
}
</code>
</pre>
  <aside class="notes">
    [FIREFOX]
    You first need to instantiate a Speech Recognition object, and since the APIs are still experimental they do need the browser prefix for Chrome (not for Firefox though)
  </aside>
</section>

<!-- SLIDE 20 -->
<section data-background-image="img/full-ink.jpg">
  <div class="name-text">Speech Recognition Attributes</div>
  <div class="name-sub emphasize">bit.ly/WebSpeechAPI</div>
<pre>
<code data-trim data-noescape>
// Sets the language to be recognized (32 languages
// supported, incl. Romanian)
recognizer.lang = 'en-US';

// Get recognition results as early as possible,
// even if they will change
recognizer.interimResults = true;

// Continuously listen to speech, regardless if
// the user takes pauses or not
recognizer.continuous = true;

// The number of alternative recognition
// matches to be returned
recognizer.maxAlternatives = 3;

…
</code>
</pre>
  <aside class="notes">
    [RETURN]
    Some of the most relevant configuration attributes of the speech recognition object are
  </aside>
</section>

<!-- SLIDE 21 -->
<section data-background-image="img/full-ink.jpg">
  <div class="name-text">Speech Recognition Event Handlers</div>
  <div class="name-sub emphasize">bit.ly/WebSpeechAPI</div>
<pre>
<code data-trim data-noescape>
recognizer.onresult // Whenever a speech recognition match is found
recognizer.onnomatch // When no match was found for the current speech
recognizer.onerror // When an error occurred
…
</code>
</pre>
  <aside class="notes">
    [ERROR]
    There are a few event handlers provided for the Speech Recognition interface, and I would recommend you check the full API at http://bit.ly/WebSpeechAPI, however the minimum needed to get you started are: onresult, onnomatch, onerror
  </aside>
</section>

<!-- SLIDE 22 -->
<section data-background-image="img/full-ink.jpg">
  <div class="name-text">Speech Recognition Control</div>
  <div class="name-sub emphasize">bit.ly/WebSpeechAPI</div>
<pre>
<code data-trim data-noescape>
recognizer.start();
recognizer.stop();
</code>
</pre>
  <aside class="notes">
    [FORMAT]
    In terms of controlling the recognizer we have a pretty self-explanatory API: start and stop.
    And before we wrap things up, I want to show you how the Speech Recognition Result format looks like.
  </aside>
</section>

<!-- SLIDE 23 -->
<section data-background-image="img/full-ink.jpg">
  <div class="name-text">Speech Recognition Result format</div>
  <div class="name-sub emphasize">bit.ly/WebSpeechAPI</div>
<pre>
<code data-trim data-noescape>
{
  isFinal: true,
  resultIndex: 3,
  results: [
    [
      { transcript: "hello" }
    ],
    [
      { transcript: "world"},
      { transcript: "word"},
      …
    ],
    …
  ]
  …
}
</code>
</pre>
  <aside class="notes">
    [STRAIGHT]
    SpeechRecognitionResult array and each result has multiple alternatives results[0][0].transcript to get the first result with highest probability
    isFinal -- this result is not an interim result and thus it will not change
    This seems quite straight forward, did anyone notice something?
  </aside>
</section>

<!-- SLIDE 24 -->
<!-- <section data-background-image="img/full-ink.jpg">
  <div class="name-text">DEMO</div>
  <div class="name-sub emphasize">Web Speech API</div>

  <div id="results">
    <span id="final_span" class="final"></span>
    <span id="interim_span" class="interim"></span>
  </div>
  <aside class="notes">
    For the past few slides I didn’t actually click anything to switch between them [PAUSE] I was basically using Speech Recognition to talk about Speech Recognition (navigate through the slides), how meta is that?

    You might be wondering can I use this now, or is it behind crazy browser flags that no user will know how to turn on to make use of your app’s speech recognition capabilities?
  </aside>
</section> -->

<!-- SLIDE 25 -->
<section data-background-image="img/full-ink.jpg" data-state="speech-end" data-transition="slide-in fade-out">
  <div class="name-text">Browser Support</div>
  <div class="name-sub emphasize">caniuse.com/#feat=speech-recognition</div>
  <i class="browser-support hero-pic">&nbsp;</i>
  <aside class="notes">
    As you can see support is not that great, it’s only Chrome and Opera that support it out of the box, whereas Firefox needs the webspeech recognition flag set. As I understand Microsoft is currently working on it.

    This might sound unfortunate, but as we all know, it takes quite a bit of time for a web API to be part of the official spec and all browsers to support it…
  </aside>
</section>

<!-- SLIDE 26 -->
<section data-background-image="img/hudl-layout-main.png" data-transition="fade" data-state="voice-visualizer speech-start">
  <h3><span class="voice-visualizer">
    <svg preserveAspectRatio="none" id="visualizer-end" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
      <defs>
        <mask id="mask-end">
          <g id="maskGroup"></g>
        </mask>
        <linearGradient id="gradient-end" x1="0%" y1="0%" x2="0%" y2="100%">
          <stop offset="0%" style="stop-color:rgba(250, 250, 250, 1.0);stop-opacity:1" />
          <stop offset="20%" style="stop-color:rgba(215, 217, 221, 1.0);stop-opacity:1" />
          <stop offset="90%" style="stop-color:rgba(255, 140, 0, 1.0);stop-opacity:1" />
          <stop offset="100%" style="stop-color:rgba(255, 99, 0, 1.0);stop-opacity:1" />
        </linearGradient>
      </defs>
      <rect x="0" y="0" width="100%" height="100%" fill="url(#gradient-end)" mask="url(#mask-end)"></rect>
    </svg>
  </span></h3>
  <div id="results">
    <span id="final_span" class="final"></span>
    <span id="interim_span" class="interim"></span>
  </div>
  <div class="name">
    <div class="name-text">Mihai Cîrlănaru</div>
    <div class="name-sub">@MCirlanaru</div>
  </div>
  <div class="main">
    <div class="main-container">
      <i class="conf-logo-secondary"></i>
    </div>
  </div>
  <aside class="notes">
    And on that note, thank you very much for listening, hope my talk inspired you to build some exciting new features in your web applications. If you have any questions, feel free to ping me on twitter, the JSConf slack, and in person.
  </aside>
</section>

      </div>
    </div>

<script src="lib/js/head.min.js"></script>
<script src="js/reveal.js"></script>

<script>

var paths = document.getElementsByTagName('path');
var visualizer = document.getElementById('visualizer');
var visualizerEnd = document.getElementById('visualizer-end');
var mask1 = visualizer.getElementById('mask');
var mask2 = visualizerEnd.getElementById('mask-end');
var path;
var NR_LINES = 40;

var soundAllowedCallback = function (stream) {
    //Audio stops listening in FF without // window.persistAudioStream = stream;
    //https://bugzilla.mozilla.org/show_bug.cgi?id=965483
    //https://support.mozilla.org/en-US/questions/984179
    window.persistAudioStream = stream;
    var audioContent = new AudioContext();
    var audioStream = audioContent.createMediaStreamSource( stream );
    var analyser = audioContent.createAnalyser();
    audioStream.connect(analyser);
    analyser.fftSize = 1024;

    var frequencyArray = new Uint8Array(analyser.frequencyBinCount);
    visualizer.setAttribute('viewBox', `0 0 ${NR_LINES} 255`);
    visualizerEnd.setAttribute('viewBox', `0 0 ${NR_LINES} 255`);

    //Through the frequencyArray has a length longer than 255, there seems to be no
    //significant data after this point. Not worth visualizing.
    for (var i = 0 ; i < NR_LINES; i++) {
        path1 = document.createElementNS('http://www.w3.org/2000/svg', 'path');
        path2 = document.createElementNS('http://www.w3.org/2000/svg', 'path');
        mask1.appendChild(path1);
        mask2.appendChild(path2);
    }
    var doDraw = function () {
        requestAnimationFrame(doDraw);
        analyser.getByteFrequencyData(frequencyArray);
        var adjustedLength;
        for (var i = 0 ; i < NR_LINES; i++) {
            adjustedLength = Math.floor(frequencyArray[i]) - (Math.floor(frequencyArray[i]) % 5);
            paths[i].setAttribute('d', 'M '+ (i) +',255 l 0,-' + adjustedLength);
        }

    }
    doDraw();
}

navigator.getUserMedia({ audio: true }, soundAllowedCallback, function(){});

var two_line = /\n\n/g;
var one_line = /\n/g;
var thank_you = /thank\s+you/gi;
function linebreak(s) {
  return s.replace(two_line, '<p></p>').replace(one_line, '<br>');
}
var first_char = /\S/;
function capitalize(s) {
  return s.replace(first_char, function(m) {
    return m.toUpperCase();
  }).replace(thank_you, '<br><span class="thanks">thank you</span>');
}

var final_transcript = '';
var slide_transcript = '';
var final_span = document.getElementById('final_span');
var interim_span = document.getElementById('interim_span');
var listeningToSpeech = false;
var recognizerTimeout = null;
var recognizer = {};

var isSpeakerNotes = document.URL.indexOf('?receiver') !== -1;

function resetSpeech() {
  final_transcript = '';
  slide_transcript = '';
  listeningToSpeech = false;
  final_span.innerHTML = '';
  interim_span.innerHTML = '';

  if (!isSpeakerNotes) {
    // Setup speech recognition
    if (recognizer.abort) recognizer.abort();

    recognizer = new webkitSpeechRecognition();
    recognizer.continuous = true;
    recognizer.lang = "en-US";
    recognizer.maxAlternatives = 3;
    recognizer.interimResults = true;
  }
}

// Initialize speech recognizer
resetSpeech();

// More info https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({
  history: true,
  controls: false,
  keyboard: {
    // ENTER KEY
    13: function() {
      if (!listeningToSpeech) {
        console.info('KEY SPEECH START');
        recognizer.start();
        listeningToSpeech = true;
      } else {
        console.info('KEY SPEECH END');
        recognizer.stop();
        listeningToSpeech = false;
      }
    },
    // SPACE KEY
    32: function() {
      recognizer.abort();
      resetSpeech();
      console.warn('RESET SPEECH RECOGNIZER');
    },
    // UP KEY
    38: 'prev',
    40: 'next'
  },
  // width: 1920,
  // height: 1080,
  width: 1280,
  height: 720,

  // More info https://github.com/hakimel/reveal.js#dependencies
  dependencies: [
    { src: 'plugin/markdown/marked.js' },
    { src: 'plugin/markdown/markdown.js' },
    { src: 'plugin/notes/notes.js', async: true },
    { src: 'plugin/zoom-js/zoom.js', async: true },
    { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
  ]
});

if (!isSpeakerNotes) {
  recognizer.onend = function(event) {
    console.warn('SPEECH END');
    listeningToSpeech = false;
  }

  recognizer.onresult = function(event) {
    var interim_transcript = '';
    for (var i = event.resultIndex; i < event.results.length; ++i) {
      if (event.results[i].isFinal) {
        final_transcript += event.results[i][0].transcript;
      } else {
        interim_transcript += event.results[i][0].transcript;
        slide_transcript += event.results[i][0].transcript;
      }

    }
    final_transcript = capitalize(final_transcript);
    final_span.innerHTML = linebreak(final_transcript);
    interim_span.innerHTML = linebreak(interim_transcript);

    console.group('SPEECH RESULT');
    console.info('INTERIM:', interim_transcript);
    console.info('FINAL:', final_transcript);
    console.groupEnd();
  };

  recognizer.onerror = function(event) {
    console.error('ERROR:', event.error);
    if (event.error === 'network' || event.error === 'aborted') {
      console.info('SPEECH RESTART');
      recognizer.stop();
      if (recognizerTimeout) {
        clearTimeout(recognizerTimeout);
        recognizerTimeout = null;
      }
      recognizerTimeout = setTimeout(function(){
        console.info('SPEECH STARTING...');
        recognizer.start();
        listeningToSpeech = true;
      }, 500);
    }
  };

  Reveal.addEventListener( 'slidechanged', function( event ) {
    // event.previousSlide, event.currentSlide, event.indexh, event.indexv
    console.info('SLIDE CHANGED', event.indexh);
  } );

  Reveal.addEventListener( 'speech-start', function(ev) {
    // Start recognition
    recognizer.start();
    listeningToSpeech = true;
    console.info('SPEECH START SLIDE')
  }, false );

  Reveal.addEventListener( 'intro', function(ev) {
    var visualizer = document.getElementById('visualizer');
    paths = document.getElementsByTagName('path');
  }, false );

  Reveal.addEventListener( 'voice-visualizer', function(ev) {
    var visualizer = document.getElementById('visualizer-end');
    paths = visualizer.getElementsByTagName('path');
  }, false );
}
</script>
  </body>
</html>
